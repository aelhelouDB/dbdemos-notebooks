{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7641be-2c3b-4714-b00a-eb9d661d5241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Production Monitoring: Automated Quality at Scale\n",
    "\n",
    "MLflow's production monitoring automatically runs quality assessments on a sample of your production traffic, ensuring your GenAI app maintains high quality standards without manual intervention. MLflow lets you use the same metrics you defined for offline evaluation in production, enabling you to have consistent quality evaluation across your entire application lifecycle - dev to prod.\n",
    "\n",
    "**Key benefits:** \n",
    "\n",
    "- Automated evaluation - Run LLM judges on production traces with configurable sampling rates\n",
    "- Continuous quality assessment - Monitor quality metrics in real-time without disrupting user experience\n",
    "- Cost-effective monitoring - Smart sampling strategies to balance coverage with computational cost\n",
    "\n",
    "Production monitoring enables you to deploy confidently, knowing that you will proactively detect issues so you can address them before they cause a major impact to your users.\n",
    "\n",
    "<img src=\"https://i.imgur.com/wv4p562.gif\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=05.production-monitoring&demo_name=ai-agent&event=VIEW\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73cae25-0ca7-4b44-873d-f052c0841d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow[databricks]>=3.1.1 databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64db55bb-4664-4b83-8f62-4a7d878e50af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/01-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "6257fc61-89a5-4a52-8ed9-fd72ec5ae6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Let's create our production grade monitor\n",
    "\n",
    "You can easily create your monitor using the UI, or directly the SDK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4d8b9d-70b2-4a46-b7d5-26e6a5c1cddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.monitoring import (\n",
    "  AssessmentsSuiteConfig,\n",
    "  GuidelinesJudge,\n",
    "  create_external_monitor,\n",
    "  get_external_monitor,\n",
    "  BuiltinJudge\n",
    ")\n",
    "import mlflow\n",
    "\n",
    "# Let's re-use our main agent experiment\n",
    "xp_name = os.getcwd().rsplit(\"/\", 1)[0]+\"/02-agent-eval/02.1_agent_evaluation\"\n",
    "mlflow.set_experiment(xp_name)\n",
    "\n",
    "\n",
    "def get_or_create_monitor():\n",
    "  try:\n",
    "    external_monitor = get_external_monitor(experiment_name=xp_name)\n",
    "  except Exception as e:\n",
    "    if \"does not exist\" in str(e):\n",
    "      # Create external monitor for automated production monitoring\n",
    "      external_monitor = create_external_monitor(\n",
    "        # Change to a Unity Catalog schema where you have CREATE TABLE permissions.\n",
    "        catalog_name=catalog,\n",
    "        schema_name=dbName,\n",
    "        assessments_config=AssessmentsSuiteConfig(\n",
    "          sample=1.0,  # sampling rate\n",
    "          assessments=[\n",
    "            # Builtin judges\n",
    "            BuiltinJudge(name=\"safety\"),\n",
    "            BuiltinJudge(name=\"groundedness\", sample_rate=0.4),\n",
    "            BuiltinJudge(name=\"relevance_to_query\"),\n",
    "            # Guidelines can refer to the request and response.\n",
    "            GuidelinesJudge(guidelines={\n",
    "              'accuracy': [\n",
    "                \"\"\"The response correctly references all factual information from the provided_info based on these rules:\n",
    "      - All factual information must be directly sourced from the provided data with NO fabrication\n",
    "      - Names, dates, numbers, and company details must be 100% accurate with no errors\n",
    "      - Meeting discussions must be summarized with the exact same sentiment and priority as presented in the data\n",
    "      - Support ticket information must include correct ticket IDs, status, and resolution details when available\n",
    "      - All product usage statistics must be presented with the same metrics provided in the data\n",
    "      - No references to CloudFlow features, services, or offerings unless specifically mentioned in the customer data\n",
    "      - AUTOMATIC FAIL if any information is mentioned that is not explicitly provided in the data\"\"\"\n",
    "              ],\n",
    "              'steps_and_reasoning': [\n",
    "                \"\"\"Reponse must be done without showing reasoning.\n",
    "      - don't mention that you need to look up things\n",
    "      - do not mention tools or function used\n",
    "      - do not tell your intermediate steps or reasoning\"\"\"\n",
    "              ]\n",
    "            })\n",
    "          ]\n",
    "        )\n",
    "      )\n",
    "  print(f\"Monitor created: {external_monitor}\")\n",
    "\n",
    "\n",
    "# monitor will create a run that will be refreshed periodically (small cost incures). \n",
    "# uncomment to create the monitor in your experiment!\n",
    "# get_or_create_monitor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bd54aaa-71b8-4082-affb-cf58bfd8ec25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05.production-monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
